import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# === 1. åŠ è½½å¹¶åˆ†å—çŸ¥è¯†åº“ ===
with open("knowledge.txt", "r", encoding="utf-8") as f:
    text = f.read()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=20,
    separators=["\n\n", "\n", ".", " ", ""]
)
chunks = text_splitter.create_documents([text])

# === 2. åˆå§‹åŒ–åµŒå…¥æ¨¡åž‹ï¼ˆæœ¬åœ°ï¼‰===
model_name = "BAAI/bge-small-en-v1.5"
embeddings = HuggingFaceBgeEmbeddings(
    model_name=model_name,
    model_kwargs={"device": "cpu"},  # å¯æ”¹ä¸º "cuda" å¦‚æžœæœ‰ GPU
    encode_kwargs={"normalize_embeddings": True}
)

# === 3. æž„å»º FAISS å‘é‡åº“ ===
vector_store = FAISS.from_documents(chunks, embeddings)

# === 4. åŠ è½½æœ¬åœ° LLMï¼ˆMistral 7B 4-bit é‡åŒ–ï¼‰===
# ä¸‹è½½æ¨¡åž‹: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF
# æ”¾åˆ° ./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
llm = LlamaCpp(
    model_path="./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
    n_ctx=2048,
    n_threads=8,        # æ ¹æ® CPU æ ¸å¿ƒæ•°è°ƒæ•´
    n_gpu_layers=0,     # è®¾ä¸º >0 è‹¥ä½¿ç”¨ GPUï¼ˆéœ€ llama-cpp æ”¯æŒ CUDAï¼‰
    verbose=False
)

# === 5. æž„å»º RAG é“¾ ===
prompt_template = """
[INST]
Use only the following context to answer the question.
If you don't know, say "I don't know based on the provided information."

Context:
{context}

Question: {question}
[/INST]
Answer:
"""

PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vector_store.as_retriever(search_kwargs={"k": 2}),
    chain_type_kwargs={"prompt": PROMPT},
    return_source_documents=True
)

# === 6. äº¤äº’å¼é—®ç­” ===
if __name__ == "__main__":
    print("âœ… Local RAG is ready! Ask any question (type 'exit' to quit):")
    while True:
        query = input("\n> ")
        if query.lower() == "exit":
            break
        result = qa_chain({"query": query})
        print("\nðŸ¤– Answer:", result["result"].strip())
        print("\nðŸ“š Sources:")
        for i, doc in enumerate(result["source_documents"], 1):
            print(f"  [{i}] {doc.page_content[:100]}...")